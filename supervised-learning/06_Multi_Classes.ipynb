{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17bf1637-101d-4172-b1f8-b540d07e18d7",
   "metadata": {},
   "source": [
    "# Multi Classes\n",
    "When a classification problem involves three or more classes, we use Multiclass Classification Strategies to adapt these binary models or employ algorithms that inherently handle multiple classes.\n",
    "\n",
    "## 1. Strategies for Binary Classifiers (Decomposition Methods)\n",
    "### 1.1. One-vs-Rest (OvR) or One-vs-All (OvA)\n",
    "This is the most common and simplest strategy. If you have $N$ classes, OvR trains $N$ separate binary classifiers.\n",
    "- Training: Each classifier is trained to distinguish one class (the \"Positive\" class) from all other classes combined (the \"Negative\" class).Example (Classes A, B, C): Train three models: Model 1 (A vs. B and C), Model 2 (B vs. A and C), Model 3 (C vs. A and B).\n",
    "- Prediction: A new sample is run through all $N$ models, and the final prediction is the class for which the corresponding model outputs the highest confidence score or probability.\n",
    "### 1.2. One-vs-One (OvO)\n",
    "OvO trains a binary classifier for every possible pair of classes.\n",
    "- Training: If you have $N$ classes, you train $\\frac{N \\times (N-1)}{2}$ classifiers.Example (Classes A, B, C): Train three models: Model 1 (A vs. B), Model 2 (A vs. C), Model 3 (B vs. C).\n",
    "- Prediction: When classifying a new sample, all models vote, and the class that receives the most votes wins.\n",
    "## 2. Algorithms with Inherent Multiclass Capability\n",
    "Some machine learning algorithms naturally handle three or more classes during their training phase:\n",
    "- Decision Trees (and Ensemble Methods like Random Forest): These models partition the feature space until the leaf nodes are pure (or mostly pure) with respect to all $N$ classes simultaneously.\n",
    "- K-Nearest Neighbors (KNN): KNN classifies a new point based on the majority class among its $K$ nearest neighbors, regardless of how many classes are present.\n",
    "- Naive Bayes: This probabilistic model calculates the probability of a data point belonging to each of the $N$ classes and selects the one with the highest probability.\n",
    "- Neural Networks: Use a softmax activation function in the output layer to directly output a probability distribution across $N$ classes.\n",
    "## 3. Python Implementation (Scikit-learn)\n",
    "In Scikit-learn, most binary classifiers (LogisticRegression, SVC, etc.) automatically use the OvR strategy when faced with a multiclass target variable, unless explicitly told otherwise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b7cefc2-e265-467c-bf52-990621da0052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Multiclass Classification Report (using OvR strategy) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        19\n",
      "  versicolor       1.00      0.92      0.96        13\n",
      "   virginica       0.93      1.00      0.96        13\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.98      0.97      0.97        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CODE CELL: Multiclass Classification (using Iris dataset which has 3 classes)\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load Iris Data (3 classes: 0, 1, 2)\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# LogisticRegression defaults to 'multi_class='auto'' which uses 'ovr' for this case\n",
    "# or 'multinomial' if the solver supports it (which is better).\n",
    "# We explicitly set 'ovr' here for demonstration.\n",
    "log_reg_ovr = LogisticRegression(solver='liblinear', multi_class='ovr', random_state=42)\n",
    "log_reg_ovr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = log_reg_ovr.predict(X_test)\n",
    "\n",
    "print(\"--- Multiclass Classification Report (using OvR strategy) ---\")\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
