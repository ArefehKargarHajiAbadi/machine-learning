{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63873bb6-63fa-401c-857b-3c9ad5d1299a",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "The core objective of classification evaluation metrics is to quantify how accurately a model predicts class labels. Since simple Accuracy can be misleading, especially with imbalanced data, we use metrics derived from the Confusion Matrix.\n",
    "## 1. The Confusion Matrix\n",
    "All key metrics start here. The Confusion Matrix is a table that summarizes the performance of a classifier by comparing the predicted classes against the actual classes.\n",
    "\n",
    "|          | Predicted Positive | Predicted Negative |\n",
    "|----------|----------|----------|\n",
    "|Actual Positive|True Positive (TP): Correctly predicted positive.|False Negative (FN): Incorrectly predicted negative (a Type II Error).|\n",
    "|Actual Negative|False Positive (FP): Incorrectly predicted positive (a Type I Error).|True Negative (TN): Correctly predicted negative.|\n",
    "\n",
    "## 2. Core Classification Metrics\n",
    "|Metric\t|Formula|\tWhat it Measures|\tWhen to Use|\n",
    "|----------|----------|----------|----------|\n",
    "|Accuracy|$$\\frac{TP + TN}{\\text{Total Samples}}$$|The overall proportion of correct predictions.|When the dataset classes are balanced.|\n",
    "Precision|$$\\frac{TP}{TP + FP}$$|Of all predictions the model made for the Positive class, how many were correct? (Focuses on minimizing False Positives).|When the cost of a False Positive is high (e.g., classifying a safe email as spam).\n",
    "|Recall (Sensitivity)|$$\\frac{TP}{TP + FN}$$|Of all actual Positive instances, how many did the model correctly identify? (Focuses on minimizing False Negatives).|When the cost of a False Negative is high (e.g., failing to detect a disease).\n",
    "|F1-Score|$$2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$|The harmonic mean of Precision and Recall.When you need a single metric that represents a balance between Precision and Recall, especially with imbalanced data.|\n",
    "\n",
    "## 3. Beyond the Matrix: ROC and AUC\n",
    "These metrics evaluate the classifier's performance across all possible decision thresholds, not just the default 0.5.\n",
    "- ROC Curve (Receiver Operating Characteristic): A plot of the True Positive Rate (Recall) against the False Positive Rate (FPR) at various threshold settings.\n",
    "\n",
    "- AUC (Area Under the Curve): Represents the probability that the model will rank a randomly chosen positive instance higher than a randomly chosen negative instance.\n",
    "\n",
    "    - AUC = 1: Perfect classification.\n",
    "\n",
    "    - AUC = 0.5: No better than random guessing.\n",
    "\n",
    "AUC is an excellent overall measure of model separability and robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ef1b467-d184-4237-bd73-70e10bb490d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE CELL 1: Setup and Prediction\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, \n",
    "    accuracy_score, \n",
    "    classification_report\n",
    ")\n",
    "from sklearn.datasets import load_breast_cancer \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- 1. Load and Prepare Data ---\n",
    "# We use the Breast Cancer dataset (binary classification)\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target \n",
    "\n",
    "# Scaling the features is generally recommended for Logistic Regression\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# --- 2. Train Model ---\n",
    "model = LogisticRegression(solver='liblinear', random_state=42) \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# --- 3. Make Predictions ---\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55946e59-d298-4f8b-8cf0-cbc2ba474592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Confusion Matrix (TN, FP, FN, TP) ---\n",
      "[[ 62   1]\n",
      " [  2 106]]\n",
      "\n",
      "Interpretation:\n",
      "True Negative (TN): 62 (Correctly predicted negative)\n",
      "False Positive (FP): 1 (Incorrectly predicted positive - Type I Error)\n",
      "False Negative (FN): 2 (Incorrectly predicted negative - Type II Error)\n",
      "True Positive (TP): 106 (Correctly predicted positive)\n",
      "\n",
      "--- 2. Overall Accuracy ---\n",
      "Accuracy: 0.9825 (Total correct predictions / Total samples)\n",
      "\n",
      "--- 3. Full Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.97      0.98      0.98        63\n",
      "      benign       0.99      0.98      0.99       108\n",
      "\n",
      "    accuracy                           0.98       171\n",
      "   macro avg       0.98      0.98      0.98       171\n",
      "weighted avg       0.98      0.98      0.98       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CODE CELL 2: Evaluation and Reporting\n",
    "\n",
    "# 1. Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"--- 1. Confusion Matrix (TN, FP, FN, TP) ---\")\n",
    "print(cm)\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"True Negative (TN): {cm[0, 0]} (Correctly predicted negative)\")\n",
    "print(f\"False Positive (FP): {cm[0, 1]} (Incorrectly predicted positive - Type I Error)\")\n",
    "print(f\"False Negative (FN): {cm[1, 0]} (Incorrectly predicted negative - Type II Error)\")\n",
    "print(f\"True Positive (TP): {cm[1, 1]} (Correctly predicted positive)\")\n",
    "\n",
    "# 2. Accuracy Score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\n--- 2. Overall Accuracy ---\")\n",
    "print(f\"Accuracy: {accuracy:.4f} (Total correct predictions / Total samples)\")\n",
    "\n",
    "# 3. Classification Report (Provides Precision, Recall, and F1-Score for each class)\n",
    "print(\"\\n--- 3. Full Classification Report ---\")\n",
    "# The report displays metrics for Class 0 (Malignant) and Class 1 (Benign)\n",
    "print(classification_report(y_test, y_pred, target_names=data.target_names))\n",
    "\n",
    "# Note: In the Classification Report above:\n",
    "# - Precision for 'benign' (class 1) is calculated as TP / (TP + FP)\n",
    "# - Recall for 'benign' (class 1) is calculated as TP / (TP + FN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
