{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db2abae4-0d16-4438-ac50-3e3362af2010",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVMs) \n",
    "SVMs are a powerful set of supervised learning models used for classification, regression, and outlier detection. They are especially effective in high-dimensional spaces and cases where the number of features exceeds the number of samples.\n",
    "## 1. The Core Concept: Maximum Margin Classifier\n",
    "At its heart, an SVM is a linear classifier that seeks to find the best possible hyperplane to separate data points belonging to different classes.\n",
    "\n",
    "- Hyperplane: In an 5$n$-dimensional feature space, a hyperplane is a flat subspace of 6$n-1$ dimensions (e.g., a line in 2D, a plane in 3D).\n",
    "- Optimal Hyperplane: The \"best\" hyperplane is the one that achieves the maximum margin between the nearest training data points of any class.\n",
    "- Margin: The distance between the hyperplane and the closest data points from either class. Maximizing this margin is crucial because it generally leads to better generalization (lower chance of overfitting) on unseen data.\n",
    "- Support Vectors: The data points that lie closest to the hyperplane (on the edge of the margin) are called the support vectors.10 These points are critical because they alone define the position and orientation of the hyperplane.\n",
    "## 2. The Kernel Trick (Non-Linear Classification)\n",
    "While the core concept is linear separation, SVMs gain immense power by being able to classify data that is not linearly separable in the original feature space. This is achieved through the Kernel Trick.\n",
    "\n",
    "- Non-Linearity: If your data is intertwined (e.g., circles within a circle), a straight line cannot separate the classes.\n",
    "\n",
    "- Kernel Function: A kernel is a function that mathematically projects the data from the low-dimensional feature space into a much higher-dimensional space where the data becomes linearly separable. The SVM then finds a linear hyperplane in this new, higher-dimensional space.\n",
    "\n",
    "\n",
    "- Popular Kernels:\n",
    "\n",
    "    - Linear: Used for linearly separable data (acts like a standard linear classifier).\n",
    "\n",
    "    - Polynomial: Used for non-linear boundaries.\n",
    "\n",
    "    - Radial Basis Function (RBF) or Gaussian: The most common kernel for general-purpose non-linear classification, creating complex boundary shapes.\n",
    "## 3. Python Implementation (SVC)\n",
    "In Scikit-learn, the primary class for classification is SVC (Support Vector Classifier). We often use the RBF kernel as a default for non-linear problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91af4969-d7ec-44ea-ba20-c103e0b88836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM (RBF Kernel) Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# CODE CELL 1: Setup and Model Training (Using a non-linear kernel)\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler # Scaling is MANDATORY for SVM!\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 0.load Data\n",
    "iris = load_iris()\n",
    "X, y= iris.data, iris.target\n",
    "\n",
    "# 1. Scaling (Crucial for SVM distance-based calculation)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 2. Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 3. Initialize and Train Model\n",
    "# C: Regularization parameter. Smaller C = wider margin, higher misclassification penalty.\n",
    "# kernel='rbf': Use the Gaussian Radial Basis Function for non-linear separation.\n",
    "svm_model = SVC(kernel='rbf', C=1.0, random_state=42) \n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# 4. Predict\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# 5. Evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"SVM (RBF Kernel) Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735a8b1f-af80-4076-9568-a2797b84b55b",
   "metadata": {},
   "source": [
    "## 4. Key Hyperparameters\n",
    ". C (Regularization Parameter)\n",
    "The parameter C controls the penalty imposed on misclassified points. It determines the trade-off between achieving a smooth decision boundary and correctly classifying the training points.\n",
    "\n",
    "Small C:\n",
    "\n",
    "Impact: Allows a larger margin, tolerating more misclassifications (smoother boundary).\n",
    "\n",
    "Result: The model is less prone to overfitting but might underfit the training data.\n",
    "\n",
    "Large C:\n",
    "\n",
    "Impact: Enforces a smaller margin, penalizing misclassifications heavily (tighter boundary).\n",
    "\n",
    "Result: The model attempts to classify all training points correctly, leading to potential overfitting.\n",
    "\n",
    "2. Gamma (RBF Kernel Only)\n",
    "The gamma parameter defines the influence radius of a single training example. It dictates how far the influence of a single data point reaches.\n",
    "\n",
    "Small Gamma:\n",
    "\n",
    "Impact: Defines a large radius of influence, making the decision boundary less constrained.\n",
    "\n",
    "Result: The resulting decision boundary is smoother and simpler.\n",
    "\n",
    "Large Gamma:\n",
    "\n",
    "Impact: Defines a small radius of influence, meaning only data points very close to the boundary affect it.\n",
    "\n",
    "Result: The boundary becomes highly convoluted, closely following the data points, which can lead to overfitting..\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b04006-f43f-4fd3-ab9a-8086e145bb40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
