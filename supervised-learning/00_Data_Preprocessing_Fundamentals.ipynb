{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f21b110-2d91-4ec9-9411-dc03bbb1122f",
   "metadata": {},
   "source": [
    "# Data Preprocessing: The Essential First Step ðŸ§¹\n",
    "Data Preprocessing is a data mining technique that transforms raw data into an understandable and usable format for machine learning algorithms. Real-world data is often incomplete, inconsistent, and dirty. If the data quality is poor, the derived results will also be unreliableâ€”a concept often summarized as \"Garbage In, Garbage Out (GIGO).\"\n",
    "\n",
    "The most critical preprocessing steps include:\n",
    "\n",
    "Handling Missing Data (NaNs).\n",
    "\n",
    "Feature Scaling (Normalization/Standardization).\n",
    "\n",
    "Encoding Categorical Variables.\n",
    "\n",
    "## 1. Dealing with Missing Data (NaN Values) \n",
    "Missing data (often represented as NaN in Pandas) occurs when no value is stored for a feature in a particular observation. How you handle missing data can significantly impact your model's performance.\n",
    "\n",
    "### 1.1 Identifying Missing Values\n",
    "The first step is always to locate and quantify the missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bb38290-ef6c-4d85-acc2-9882b73445bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "    Age   Salary  Gender Purchased\n",
      "0  35.0  50000.0    Male        No\n",
      "1  27.0  48000.0  Female       Yes\n",
      "2  40.0      NaN  Female        No\n",
      "3  21.0  52000.0    Male        No\n",
      "4  44.0  55000.0     NaN       Yes\n",
      "5  48.0  79000.0    Male        No\n",
      "6  38.0  62000.0  Female        No\n",
      "7   NaN  83000.0    Male       Yes\n",
      "\n",
      "Missing Values Count:\n",
      "Age          1\n",
      "Salary       1\n",
      "Gender       1\n",
      "Purchased    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# CODE CELL 1: Setup and Loading a Dataset with Known Missing Values\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "\n",
    "# Create a sample dataset with missing values\n",
    "data = \"\"\"\n",
    "Age,Salary,Gender,Purchased\n",
    "35,50000,Male,No\n",
    "27,48000,Female,Yes\n",
    "40,,Female,No\n",
    "21,52000,Male,No\n",
    "44,55000,,Yes\n",
    "48,79000,Male,No\n",
    "38,62000,Female,No\n",
    ",83000,Male,Yes\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_csv(StringIO(data))\n",
    "print(\"Original Data:\")\n",
    "print(df)\n",
    "# Check for missing values in each column\n",
    "print(\"\\nMissing Values Count:\")\n",
    "print(df.isnull().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08397d2-e31d-424b-912f-b94d89edcee3",
   "metadata": {},
   "source": [
    "### 1.2 Strategies for Imputation\n",
    "There are two primary methods for addressing missing data:\n",
    "\n",
    "- Deletion: Removing rows or columns entirely.\n",
    "\n",
    "- Imputation: Filling in the missing values.\n",
    "\n",
    "**Strategy A: Deletion**\n",
    "- Listwise Deletion (Removing Rows): If only a few rows have missing data, you can delete those rows. This is quick but can lead to data loss if many rows are affected.\n",
    "\n",
    "- Deleting Columns: If a feature (column) is missing a value in a majority of the observations, it may be best to drop the entire feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bfd059c-f1ad-4441-8f00-6759fbc943ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data after Row Deletion:\n",
      "    Age   Salary  Gender Purchased\n",
      "0  35.0  50000.0    Male        No\n",
      "1  27.0  48000.0  Female       Yes\n",
      "3  21.0  52000.0    Male        No\n",
      "5  48.0  79000.0    Male        No\n",
      "6  38.0  62000.0  Female        No\n"
     ]
    }
   ],
   "source": [
    "# CODE CELL 2: Deleting Rows with Missing Values (Use with caution!)\n",
    "df_deleted = df.dropna()\n",
    "print(\"\\nData after Row Deletion:\")\n",
    "print(df_deleted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a01220-8387-4a80-a761-9254c9148e7f",
   "metadata": {},
   "source": [
    "**Strategy B: Imputation**\n",
    "Imputation means estimating the missing value based on the available data in that feature.\n",
    "\n",
    "- Mean/Median Imputation (for Numerical Data):\n",
    "\n",
    "     - Mean: Good for normally distributed data. However, it is sensitive to outliers.\n",
    "\n",
    "    - Median: Better for data with outliers or skewed distributions.\n",
    "\n",
    "- Mode Imputation (for Categorical Data): Using the most frequent category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04cac5eb-51d3-4d0c-af6a-51604ea2eccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data after Imputation:\n",
      "    Age        Salary  Gender Purchased\n",
      "0  35.0  50000.000000    Male        No\n",
      "1  27.0  48000.000000  Female       Yes\n",
      "2  40.0  61285.714286  Female        No\n",
      "3  21.0  52000.000000    Male        No\n",
      "4  44.0  55000.000000    Male       Yes\n",
      "5  48.0  79000.000000    Male        No\n",
      "6  38.0  62000.000000  Female        No\n",
      "7  38.0  83000.000000    Male       Yes\n",
      "\n",
      "Missing Values Count after Imputation:\n",
      "Age          0\n",
      "Salary       0\n",
      "Gender       0\n",
      "Purchased    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# CODE CELL 3: Mean/Median Imputation using Scikit-learn's SimpleImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# 1. Impute 'Age' (Numerical, less sensitive to outliers) with the Median\n",
    "imputer_age = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "df['Age'] = imputer_age.fit_transform(df[['Age']])\n",
    "\n",
    "# 2. Impute 'Salary' (Numerical) with the Mean\n",
    "imputer_salary = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "df['Salary'] = imputer_salary.fit_transform(df[['Salary']])\n",
    "\n",
    "# 3. Impute 'Gender' (Categorical) with the Most Frequent value (Mode)\n",
    "imputer_gender = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "df['Gender'] = imputer_gender.fit_transform(df[['Gender']]).flatten()\n",
    "\n",
    "print(\"\\nData after Imputation:\")\n",
    "print(df)\n",
    "print(\"\\nMissing Values Count after Imputation:\")\n",
    "print(df.isnull().sum()) # Should show 0 for all columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab6afd6-7cf7-4898-a1c4-2ae31fbe495d",
   "metadata": {},
   "source": [
    "## 2. Feature Scaling \n",
    "Once missing data is handled, we often need to scale the features. This is critical for algorithms that calculate distances (like KNN or SVM) or rely on gradient descent (like Linear Regression and Neural Networks).\n",
    "\n",
    "**Normalization (Min-Max Scaling):** Rescales the data to a fixed range, usually 0 to 1.\n",
    "\n",
    "**Standardization (Z-score):** Rescales the data to have a mean of 0 and a standard deviation of 1\n",
    "## 1. Setup and Sample Data\n",
    "We start with a simple numerical dataset where the features have different scales, making scaling necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f090dd97-c674-445d-9346-25e986f46090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Original Training Data (X_train) ---\n",
      "   Age  Salary\n",
      "2   45  120000\n",
      "0   25   30000\n",
      "3   22   20000\n"
     ]
    }
   ],
   "source": [
    "# CODE CELL 1: Setup and Sample Data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Sample Data: Features are on different scales (Age vs. Salary)\n",
    "data = {\n",
    "    'Age': [25, 30, 45, 22, 58],\n",
    "    'Salary': [30000, 60000, 120000, 20000, 90000],\n",
    "    'Target': [0, 1, 1, 0, 1]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Separate features (X) from the target (y)\n",
    "X = df[['Age', 'Salary']]\n",
    "y = df['Target']\n",
    "\n",
    "# Split data (Crucial: Fit scaler only on training data!)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "print(\"--- Original Training Data (X_train) ---\")\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f7065a-6da3-4e79-818c-a90854b7152d",
   "metadata": {},
   "source": [
    "## 2. Standardization (StandardScaler)\n",
    "Standardization (or Z-score normalization) transforms data such that the resulting distribution has a mean of 0 and a standard deviation of 1.$$\\mathbf{x}_{\\text{scaled}} = \\frac{x - \\mu}{\\sigma}$$This is generally preferred for algorithms that assume a normal distribution or when your data has outliers, as it preserves information about outliers better than Min-Max Scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73187ec9-f3ff-4fd9-9e3a-6b5715a0feb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Standardized Training Data (Mean=0, Std=1) ---\n",
      "       Age    Salary\n",
      "2  1.40400  1.408374\n",
      "0 -0.55507 -0.592999\n",
      "3 -0.84893 -0.815374\n",
      "\n",
      "Mean of 'Age' after standardization: -0.00\n",
      "Standard Deviation of 'Age': 1.22\n"
     ]
    }
   ],
   "source": [
    "# CODE CELL 2: Standardization\n",
    "# 1. Create the StandardScaler object\n",
    "scaler_standard = StandardScaler()\n",
    "\n",
    "# 2. Fit the scaler on the TRAINING data ONLY\n",
    "scaler_standard.fit(X_train)\n",
    "\n",
    "# 3. Transform both the training and test sets\n",
    "X_train_scaled_standard = scaler_standard.transform(X_train)\n",
    "X_test_scaled_standard = scaler_standard.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrame for better viewing\n",
    "X_train_scaled_standard_df = pd.DataFrame(\n",
    "    X_train_scaled_standard, \n",
    "    columns=X_train.columns, \n",
    "    index=X_train.index\n",
    ")\n",
    "\n",
    "print(\"\\n--- Standardized Training Data (Mean=0, Std=1) ---\")\n",
    "print(X_train_scaled_standard_df)\n",
    "print(f\"\\nMean of 'Age' after standardization: {X_train_scaled_standard_df['Age'].mean():.2f}\")\n",
    "print(f\"Standard Deviation of 'Age': {X_train_scaled_standard_df['Age'].std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98dafd2-6b20-427c-aca3-916555175aa8",
   "metadata": {},
   "source": [
    "## 3. Normalization (MinMaxScaler)\n",
    "**Normalization**(or Min-Max Scaling) rescales the data so that all values fall within a specific range, usually 0 to 1.$$\\mathbf{x}_{\\text{scaled}} = \\frac{x - x_{\\text{min}}}{x_{\\text{max}} - x_{\\text{min}}}$$This is useful when you need bounded data (e.g., for certain Neural Network activation functions). However, it is very sensitive to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8539a2d7-ef29-4fd7-81a3-e16c97ca237e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Normalized Training Data (Min=0, Max=1) ---\n",
      "        Age  Salary\n",
      "2  1.000000     1.0\n",
      "0  0.130435     0.1\n",
      "3  0.000000     0.0\n",
      "\n",
      "Minimum of 'Age' after normalization: 0.00\n",
      "Maximum of 'Age' after normalization: 1.00\n"
     ]
    }
   ],
   "source": [
    "# CODE CELL 3: Normalization (Min-Max Scaling)\n",
    "# 1. Create the MinMaxScaler object\n",
    "\n",
    "scaler_minmax = MinMaxScaler()\n",
    "# 2. Fit the scaler on the TRAINING data ONLY\n",
    "scaler_minmax.fit(X_train)\n",
    "\n",
    "# 3. Transform both the training and test sets\n",
    "X_train_scaled_minmax = scaler_minmax.transform(X_train)\n",
    "X_test_scaled_minmax = scaler_minmax.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrame for better viewing\n",
    "X_train_scaled_minmax_df = pd.DataFrame(\n",
    "    X_train_scaled_minmax, \n",
    "    columns=X_train.columns, \n",
    "    index=X_train.index\n",
    ")\n",
    "\n",
    "print(\"\\n--- Normalized Training Data (Min=0, Max=1) ---\")\n",
    "print(X_train_scaled_minmax_df)\n",
    "print(f\"\\nMinimum of 'Age' after normalization: {X_train_scaled_minmax_df['Age'].min():.2f}\")\n",
    "print(f\"Maximum of 'Age' after normalization: {X_train_scaled_minmax_df['Age'].max():.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
