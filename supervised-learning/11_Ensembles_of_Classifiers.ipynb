{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "852dc221-b891-4f01-ac5d-66de42f772b7",
   "metadata": {},
   "source": [
    "# Ensembles of classifiers\n",
    "Ensembles of classifiers, often referred to as Ensemble Learning, are a machine learning technique where multiple models (called base estimators or weak learners) are trained and their predictions are combined to achieve better predictive performance than could be obtained from any single model.\n",
    "\n",
    "The core idea is that a group of diverse, individual models can leverage their collective intelligence to reduce common errors like variance and bias, leading to improved accuracy and robustness.\n",
    "## Main Types of Ensemble Methods\n",
    "Ensemble methods are typically categorized based on how the base models are trained (in parallel or sequentially) and how their predictions are combined.\n",
    "| Method | Training Type | Key Principle | Common Algorithm Example | Primary Focus |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| **Bagging** | Parallel | Trains multiple identical base learners on **bootstrap samples** (random subsets with replacement). Predictions are combined via **Voting** or **Averaging**. | **Random Forest** (uses Decision Trees) | Reduces **Variance** (addressing overfitting). |\n",
    "| **Boosting** | Sequential | Trains base learners sequentially, where each new model is built to **correct the errors** made by the previous ones (often by re-weighting samples). | **AdaBoost**, **Gradient Boosting Machines (GBM)**, **XGBoost** | Reduces **Bias** (turning weak learners into a strong learner). |\n",
    "| **Stacking** | Parallel/Meta | Trains diverse base learners. A separate **meta-model** (or second-level model) learns to optimally combine their predictions. | Uses a simpler model (e.g., Logistic Regression) as the meta-model over base models like SVMs and Decision Trees. | Improves **Overall Accuracy** by leveraging diverse model strengths. |\n",
    "\n",
    "## Random Forest Classifier (the ensemble model)\n",
    "A Random Forest is an ensemble classifier that operates by constructing a multitude of decision trees at training time. For classification, the output of the forest is the class chosen by most trees (majority voting).\n",
    "\n",
    "- Bagging: Each tree is trained on a random subset of the data (sampled with replacementâ€”bootstrapping).\n",
    "\n",
    "- Feature Randomness: When splitting a node in a tree, only a random subset of the features is considered, ensuring diversity among the trees.\n",
    "\n",
    "This dual source of randomness helps the Random Forest significantly reduce variance and prevent overfitting, often making it one of the most reliable out-of-the-box machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0da147e-d0d4-476c-b1d1-c880fea21480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Random Forest Classifier: 1.0000\n",
      "\n",
      "First 5 Predicted Labels: [1 0 2 1 1]\n",
      "First 5 True Labels:      [1 0 2 1 1]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier # The ensemble classifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Load Data\n",
    "# Using the built-in Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data  # Features\n",
    "y = iris.target # Target (Class Labels)\n",
    "\n",
    "# 2. Split Data\n",
    "# Split the data into training and testing sets (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Initialize and Train the Ensemble Model\n",
    "# Create a Random Forest Classifier instance (an ensemble of Decision Trees)\n",
    "# n_estimators=100 means it will build 100 individual decision trees\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model using the training data\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# 4. Predict\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# 5. Evaluate the Model\n",
    "# Calculate and print the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of Random Forest Classifier: {accuracy:.4f}\")\n",
    "print(\"\\nFirst 5 Predicted Labels:\", y_pred[:5])\n",
    "print(\"First 5 True Labels:     \", y_test[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
