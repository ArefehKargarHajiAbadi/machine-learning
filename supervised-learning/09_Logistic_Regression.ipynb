{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71c1c47b-26b5-4e56-889a-1e3ce0b5293c",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "Logistic Regression is one of the most fundamental algorithms used for binary classification tasks in machine learning. Despite the name, it is a classification algorithm, not a regression one. It estimates the probability that an observation belongs to a particular class (e.g., probability of success/failure, yes/no).\n",
    "\n",
    "## 1. The Mathematics: Sigmoid Function\n",
    "Logistic Regression uses the Sigmoid Function (or Logistic Function) to transform the result of a linear equation into a probability value between 0 and 1.\n",
    "\n",
    "### 1.1. The Linear Component\n",
    "First, like Linear Regression, it computes a linear combination of the input features ($\\mathbf{x}$) and their corresponding weights ($\\mathbf{\\beta}$):$$z = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n = \\mathbf{\\beta}^\\top \\mathbf{x}$$\n",
    "### 1.2. The Sigmoid Transformation\n",
    "This linear output ($z$) is called the log-odds or logit. To convert $z$, which can range from $-\\infty$ to $+\\infty$, into a probability $P(y=1|\\mathbf{x})$ (where $y=1$ is the desired class), it is passed through the Sigmoid function ($\\sigma$):$$\\sigma(z) = P(y=1|\\mathbf{x}) = \\frac{1}{1 + e^{-z}}$$\n",
    "- If $z$ is a large positive number, $P \\approx 1$.\n",
    "- If $z = 0$, $P = 0.5$.\n",
    "- If $z$ is a large negative number, $P \\approx 0$.\n",
    "### 1.3. Decision Boundary\n",
    "The model classifies an observation based on a threshold, typically 0.5:If $P(y=1|\\mathbf{x}) \\geq 0.5$, the observation is classified as 1 (Positive Class).If $P(y=1|\\mathbf{x}) < 0.5$, the observation is classified as 0 (Negative Class).\n",
    "## 2. Python Implementation with Scikit-learn\n",
    "Logistic Regression requires feature scaling if it is solved using gradient-based optimizers, although the liblinear solver in Scikit-learn often performs well without it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b05c4f2c-09f6-4fd6-a5d3-314bff4c97d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE CELL 1: Setup and Model Training\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.datasets import load_breast_cancer # A binary classification dataset\n",
    "\n",
    "# 1. Load Data\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# 2. Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 3. Initialize and Train Model\n",
    "# solver='liblinear' is a good choice for smaller datasets\n",
    "model = LogisticRegression(solver='liblinear', random_state=42) \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 4. Predict\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1] # Probability of the positive class (1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5162fa81-bd47-4413-a16f-af3d6d3aebce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Confusion Matrix ---\n",
      "[[ 59   4]\n",
      " [  2 106]]\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.97      0.94      0.95        63\n",
      "      benign       0.96      0.98      0.97       108\n",
      "\n",
      "    accuracy                           0.96       171\n",
      "   macro avg       0.97      0.96      0.96       171\n",
      "weighted avg       0.96      0.96      0.96       171\n",
      "\n",
      "\n",
      "Model Coefficients (Weights):\n",
      "[ 2.17531613e+00  1.59671161e-01 -1.25366787e-01 -4.00239206e-03\n",
      " -1.30406138e-01 -4.11269744e-01 -6.55017223e-01 -3.50092192e-01\n",
      " -2.02213738e-01 -2.92902947e-02 -6.61118882e-02  1.40363036e+00\n",
      "  1.17862799e-01 -1.09266066e-01 -1.46457567e-02 -2.48430128e-02\n",
      " -6.34943239e-02 -4.11473924e-02 -4.87792139e-02 -7.70259549e-04\n",
      "  1.15521174e+00 -3.90337252e-01 -7.67977046e-02 -2.13240168e-02\n",
      " -2.42133804e-01 -1.13976190e+00 -1.57932792e+00 -6.17336578e-01\n",
      " -7.29100874e-01 -1.10784729e-01]\n"
     ]
    }
   ],
   "source": [
    "# CODE CELL 2: Evaluation\n",
    "# Display the Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"--- Confusion Matrix ---\")\n",
    "print(cm)\n",
    "\n",
    "# Display the full Classification Report (Accuracy, Precision, Recall, F1-Score)\n",
    "print(\"\\n--- Classification Report ---\")\n",
    "print(classification_report(y_test, y_pred, target_names=data.target_names))\n",
    "\n",
    "# Print the model coefficients (for interpretability)\n",
    "print(\"\\nModel Coefficients (Weights):\")\n",
    "# The coefficients show the impact of each feature on the log-odds of the target event.\n",
    "print(model.coef_[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
